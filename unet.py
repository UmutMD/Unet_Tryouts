# -*- coding: utf-8 -*-
"""Unet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulnT4GTdhNIeKXZIzVPYU4jj8C-sLfI6

Mounting Database from drive

https://www.kaggle.com/competitions/severstal-steel-defect-detection/overview
"""

from google.colab import drive
drive.mount('/content/drive/')

from zipfile import ZipFile
file_name='/content/drive/MyDrive/severstal-steel-defect-detection.zip'
with ZipFile(file_name,'r') as Zip:
  Zip.extractall()

"""Libraries and Dependencies + tensorflow, keras

"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -U tensorflow-addons

import pandas as pd
import numpy as np
import cv2
import time
import tensorflow
import matplotlib.pyplot as plt
import seaborn as sb
import os
import math
from random import random
import tensorflow as tf
import keras
from PIL import Image
from tensorflow.keras import backend as K
from keras.layers import Input, Lambda, Flatten,Conv2D,Dense,MaxPooling2D,Dropout,BatchNormalization,GlobalAveragePooling2D,Activation,Add,UpSampling2D,Concatenate,LeakyReLU,Conv2DTranspose,concatenate
from keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard
import tensorflow_addons as tfa
import datetime
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from keras import backend as K
from keras.layers import Layer
from tensorflow.keras.applications import EfficientNetB0

# %load_ext tensorboard

"""the database is loaded ?

"""

df=pd.read_csv('train.csv')
print(df.shape)
df.head(2)

#https://stackoverflow.com/questions/58693261/create-a-rle-run-lenth-encoding-mask-with-tensorflow-datasets


def rle2mask(mask_rle):
  shape = tf.convert_to_tensor((1600,256), tf.int32)
  size = tf.math.reduce_prod(shape)
  s = tf.strings.split(mask_rle)
  s = tf.strings.to_number(s, tf.int32)
  starts = s[::2] - 1
  lens = s[1::2]
  total_ones = tf.reduce_sum(lens)
  ones = tf.ones([total_ones], tf.uint8)
  r = tf.range(total_ones)
  lens_cum = tf.math.cumsum(lens)
  s = tf.searchsorted(lens_cum, r, 'right')
  idx = r + tf.gather(starts - tf.pad(lens_cum[:-1], [(1, 0)]), s)
  mask_flat = tf.scatter_nd(tf.expand_dims(idx, 1), ones, [size])
  mask  = tf.transpose(tf.reshape(mask_flat, shape)[::2,::])
  return mask

"""Dice Coef and Loss
0.0000000001

https://www.kaggle.com/code/yerramvarun/understanding-dice-coefficient/notebook

"""

smooth = 1e-10
def dice_coef(y_true, y_pred):
    y_true =  tf.cast(y_true,'float32')
    y_pred =  tf.cast(y_pred,'float32')
    y_true = tf.keras.layers.Flatten()(y_true)
    y_pred = tf.keras.layers.Flatten()(y_pred)
    intersection = tf.reduce_sum(y_true * y_pred)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)

def dice_loss(y_true, y_pred):
    return 1.0 - dice_coef(y_true, y_pred)

def decoder(filter,X,layer_name,skip=True):
        x = UpSampling2D((2, 2))(X)
        if skip==True:
          x_skip = encoder.get_layer(layer_name).output
          x = Concatenate()([x, x_skip])
        
        x = Conv2D(filter, (3, 3), padding='same')(x)
        x = BatchNormalization()(x)
        x= Dropout(0.25)(x)
        x = Activation('relu')(x)
        
        x = Conv2D(filter, (3, 3), padding='same')(x)
        x = BatchNormalization()(x)
        x= Dropout(0.15)(x)
        x = Activation('relu')(x)
        return x

inputs = Input(shape=(256, 800, 3))
encoder = EfficientNetB0(input_tensor=inputs, weights="imagenet", include_top=False)

x=decoder(256,encoder.output,'block6a_expand_activation',skip=True)# filter size, prvious layer intput, concat layer name, skip=True
x=decoder(128,x,'block4a_expand_activation',skip=True)
x=decoder(64,x,'block3a_expand_activation',skip=True)
x=decoder(32,x,'block2a_expand_activation',skip=True)
x=decoder(16,x,'None',skip=False)

x=Conv2D(1,3,padding='same')(x)
out=Activation('sigmoid')(x)

model=Model(inputs=inputs,outputs=out)

model.summary()

Unet Model.

# input_shape = (256, 1600, 1)
#inputs = Input(input_shape)

#c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)
#c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)
#p1 = MaxPooling2D((2, 2)) (c1)

#c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)
#c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)
#p2 = MaxPooling2D((2, 2)) (c2)

#c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)
#c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)
#p3 = MaxPooling2D((2, 2)) (c3)

#c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)
#c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)
#p4 = MaxPooling2D(pool_size=(2, 2)) (c4)

#c5 = Conv2D(64, (3, 3), activation='relu', padding='same') (p4)
#c5 = Conv2D(64, (3, 3), activation='relu', padding='same') (c5)
#p5 = MaxPooling2D(pool_size=(2, 2)) (c5)

#c55 = Conv2D(128, (3, 3), activation='relu', padding='same') (p5)
#c55 = Conv2D(128, (3, 3), activation='relu', padding='same') (c55)

#u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c55)
#u6 = concatenate([u6, c5])
#c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)
#c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)

#u71 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)
#u71 = concatenate([u71, c4])
#c71 = Conv2D(32, (3, 3), activation='relu', padding='same') (u71)
#c61 = Conv2D(32, (3, 3), activation='relu', padding='same') (c71)

#u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c61)
#u7 = concatenate([u7, c3])
#c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)
#c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)

#u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)
#u8 = concatenate([u8, c2])
#c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)
#c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)

#u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)
#u9 = concatenate([u9, c1], axis=3)
#c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)
#c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)

#outputs = Conv2D(4, (1, 1), activation='sigmoid') (c9)

#model = Model(inputs=[inputs], outputs=[outputs])
#model.summary()
#

"""Data Augment Function, includes flips and contrast """

def augment(image, mask):
    a=random()
    if a>0.2 and a<0.3:
      image = tf.image.random_flip_left_right(image)
      image = tf.clip_by_value(image, 0.0, 1.0)

      mask = tf.image.random_flip_left_right(mask)
      mask = tf.clip_by_value(image, 0.0, 1.0)
      return image, mask
    
  
    if a>0.45 and a<0.6:
      image=tf.image.random_flip_up_down(image)
      image = tf.clip_by_value(image, 0.0, 1.0)

      mask=tf.image.random_flip_up_down(mask)
      mask = tf.clip_by_value(mask, 0.0, 1.0)
      return image, mask
    if a>0.65 and a<0.75:
      image=tf.image.random_contrast(image,0.6,1.4)
      image = tf.clip_by_value(image, 0.0, 1.0)

      mask=tf.image.random_contrast(mask,0.6,1.4)
      mask = tf.clip_by_value(mask, 0.0, 1.0)
      return image, mask
      
    else:
      image = tf.clip_by_value(image, 0.0, 1.0)
      return image, mask

"""Preprossecing image to float and 256 x 800 """

def preprocess_image(filename,mask_file):
    #image preprocessing
    image_string = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(image_string, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float16)
    image = tf.image.resize(image, [256,800])
    image = tf.image.convert_image_dtype(image, tf.float16)
    
    #mask preprocessing
    mask=rle2mask(mask_file)
    mask=tf.expand_dims(mask,2)
    return image, mask

"""https://www.tensorflow.org/api_docs/python/tf/dtypes/DType 

https://www.programcreek.com/python/example/90378/tensorflow.uint8

https://github.com/tensorflow/probability/issues/1218
"""

def tfdata_train(filenames,mask_file,batch_size):
  dataset = tf.data.Dataset.from_tensor_slices((filenames,mask_file))
  dataset= dataset.shuffle(len(filenames))
  #convert file path name to image array and ask array
  dataset= dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
  #augmentaion
  dataset = dataset.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)  
  return dataset

def tfdata_test(filenames,mask_file,batch_size):
  dataset = tf.data.Dataset.from_tensor_slices((filenames,mask_file))
  dataset = dataset.shuffle(len(filenames))
  dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
  #dataset=dataset.repeat()
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  return dataset

def tf_test(filenames,mask_file):
  dataset = tf.data.Dataset.from_tensor_slices((filenames, mask_file))
  dataset = dataset.map(preprocess_image)
  dataset = dataset.batch(1)
  return dataset

"""FUNCTION FOR TRAINGING DATA

Forward propagation, loss - gradient and apply gradients 
Adam default learning rate
"""

def training(X_train_seg,X_val_seg,bs,epoch,path,train_steps,validation_steps):
  #loading model
  model=Model(inputs=inputs,outputs=out)
  #batch size
  BATCH_SIZE=bs
  iters = math.ceil(100/BATCH_SIZE) 
  #epoch
  EPOCHS=epoch
  #train step function
  @tf.function
  def train_step(input_vector, output_vector):
      with tf.GradientTape() as tape:
          #forward propagation
          output_predicted = model(inputs=input_vector, training=True)
          #loss
          loss = dice_loss(output_vector, output_predicted)
      #getting gradients
      gradients = tape.gradient(loss, model.trainable_variables)
      #applying gradients
      optimizer.apply_gradients(zip(gradients, model.trainable_variables))
      return loss, output_predicted, gradients

  ##validation step function
  @tf.function
  def val_step(input_vector, output_vector):
      #getting output of validation data
      output_predicted = model(inputs=input_vector, training=False)
      #loss calculation
      loss = dice_loss(output_vector, output_predicted)
      return loss, output_predicted

  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

  train_loss = tf.keras.metrics.Mean(name='train_loss')
  val_loss = tf.keras.metrics.Mean(name='val_loss')
  train_metric = tf.keras.metrics.Mean(name="train_dice_coff ")
  val_metric = tf.keras.metrics.Mean(name="val_metric")

  #path='/content/drive/My Drive/defect1_seg_model/'
  wtrain = tf.summary.create_file_writer(logdir=path+'logs/train')
  wval = tf.summary.create_file_writer(logdir=path+'logs/val')
  
  
  for epoch in range(EPOCHS):
      
      #resetting the states of the loss and metrics
      train_loss.reset_states()
      val_loss.reset_states()
      train_metric.reset_states()
      val_metric.reset_states()
      
      ##counter for train loop iteration
      counter = 0
      
      #lists to save true and validation data. 
      train_true = []
      train_predicted = []
      val_true = []
      val_predicted = []

      #ietrating over train data batch by batch
      for image,mask in X_train_seg:
          #train step
          loss_, pred_out, gradients = train_step(image, mask)
          #adding loss to train loss
          train_loss(loss_)
          #counting the step number
          temp_step = epoch*iters+counter
          counter = counter + 1
          
          #calculating dice coff  for batch
          batch_metric = dice_coef(mask, pred_out)
          train_metric(batch_metric)
          
          #appending it to list
          train_predicted.append(pred_out)
          train_true.append(mask)
          
          ##tensorboard 
          with tf.name_scope('per_step_training'):
              with wtrain.as_default():
                  tf.summary.scalar("batch_loss", loss_, step=temp_step)
                  tf.summary.scalar('batch_metric', batch_metric, step=temp_step)
          with tf.name_scope("per_batch_gradients"):
              with wtrain.as_default():
                  for i in range(len(model.trainable_variables)):
                      name_temp = model.trainable_variables[i].name
                      tf.summary.histogram(name_temp, gradients[i], step=temp_step)
          if counter%train_steps==0 and counter!=0:
            break

      #saving model    
      filename=path+'weights_'+str(epoch)+'.hdf5'
      tf.saved_model.save(model, filename)

      #calculating the final loss and metric
      train_true = tf.concat(train_true, axis=0)
      train_predicted = tf.concat(train_predicted, axis=0)
      train_loss_final = dice_loss(train_true, train_predicted)
      train_metric_dice_coef = dice_coef(train_true, train_predicted)
    
      counter = 0
      for image_val,mask_val in X_val_seg:
          loss_val, pred_out_val = val_step(image_val, mask_val)
          #appending to lists
          val_true.append(mask_val)
          val_predicted.append(pred_out_val)
          val_loss(loss_val)
          #calculating metric
          batch_metric_val = dice_coef(mask_val, pred_out_val)
          val_metric(batch_metric_val)
          #validation_steps
          counter = counter + 1
          if counter%validation_steps==0 and counter!=0:
            break
          
      
      
      #calculating final loss and metric   
      val_true = tf.concat(val_true, axis=0)
      val_predicted = tf.concat(val_predicted, axis=0)
      val_loss_final = dice_loss(val_true, val_predicted)
      val_metric_dice_coef = dice_coef(val_true, val_predicted)
      
      #printing loss and metrics valve
      template = '''Epoch {}, Train Loss: {:0.6f}, Mean batch Train Loss: {:0.6f}, dice coef: {:0.5f}, Mean batch Train dice coff: {:0.5f},
      Val Loss: {:0.6f}, Mean batch Val Loss: {:0.6f}, Val dice coef: {:0.5f}, Mean batch Val dice coef: {:0.5f}'''
      
      print(template.format(epoch+1, train_loss_final.numpy(), train_loss.result(), 
                            train_metric_dice_coef, train_metric.result(), val_loss_final.numpy(),
                            val_loss.result(), val_metric_dice_coef, val_metric.result()))
      print('-'*30)
      
      #tensorboard
      with tf.name_scope("per_epoch_loss_metric"):
          with wtrain.as_default():
              tf.summary.scalar("mean_loss", train_loss.result().numpy(), step=epoch)
              tf.summary.scalar('loss', train_loss_final.numpy(), step=epoch)
              tf.summary.scalar('metric', train_metric_dice_coef, step=epoch)
              tf.summary.scalar('mean_metric', train_metric.result().numpy(), step=epoch)
          with wval.as_default():
              tf.summary.scalar('mean_loss', val_loss.result().numpy(), step=epoch)
              tf.summary.scalar('loss', val_loss_final.numpy(), step=epoch)
              tf.summary.scalar('metric', val_metric_dice_coef, step=epoch)
              tf.summary.scalar('mean_metric', val_metric.result().numpy(), step=epoch)

def visual_image(model):
  for j in test.take(3):
      #orginal steel image
      image=j[0][0]
      #orginal mask
      mask=j[1][0,:,:,0]
      #predict mask
      predict=model.predict(j[0])[0,:,:,0]

     
      plt.figure(figsize=(10,10))
      plt.subplot(131)
      plt.imshow(image)
      plt.subplot(132)
      plt.imshow(mask)
      plt.subplot(133)
      plt.imshow(predict)
      plt.show()

seg_type_1_data=df[df.ClassId==1]

seg_type_1_data.head()

seg_type_1_data['ImageId']=['train_images/'+i for i in seg_type_1_data.ImageId]
seg_type_1_data.head()

X_train, X_test = train_test_split(seg_type_1_data, test_size = 0.15,random_state=42)
X_train, X_val = train_test_split(X_train, test_size = 0.16,random_state=42)
print('train shape : ',X_train.shape)
print('test shape : ', X_test.shape)
print('val shape : ', X_val.shape)

X_train.head()

X_train_seg_1 = tfdata_train(X_train.ImageId.tolist(), X_train.EncodedPixels.tolist(), 8)
X_val_seg_1 = tfdata_test(X_val.ImageId.tolist(),X_val.EncodedPixels.tolist(), 8)
X_train_seg_1, X_val_seg_1

"""https://stackoverflow.com/questions/42319337/tensorflow-type-error-value-passed-to-parameter-shape-has-datatype-float32-no


https://github.com/keras-team/keras/issues/6171 
"""

path='/content/drive/MyDrive/defect1_seg_model/'
training(X_train_seg_1, X_val_seg_1, 16, 3, path, 256,800)

#plt.figure(figsize=(12,12))
#plt.imshow(plt.imread('/content/drive/MyDrive/Image_CS@2/seg_1.png'))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/My\ Drive/defect1_seg_model/

po='/content/drive/MyDrive/defect1_seg_model/weights_5.hdf5'
model.save('/content/drive/MyDrive/defect1_seg_model/weights_5.hdf5')
model1=tf.keras.models.load_model(filepath=po)

test=tf_test(X_test.ImageId.tolist(),X_test.EncodedPixels.tolist())
train=tf_test(X_train.ImageId.tolist(),X_train.EncodedPixels.tolist())
val=tf_test(X_val.ImageId.tolist(),X_val.EncodedPixels.tolist())

model1.compile(optimizer='adam',loss=dice_loss,metrics=[dice_coef])

pd.DataFrame(np.array(model1.evaluate(test)).reshape(1,-1),columns=['test_loss','test_dice_coef'])

pd.DataFrame(np.array(model1.evaluate(val)).reshape(1,-1),columns=['val_loss','val_dice_coef'])

pd.DataFrame(np.array(model1.evaluate(train)).reshape(1,-1),columns=['train_loss','train_dice_coef'])

visual_image(model1)